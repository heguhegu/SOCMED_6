{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo Day Socmed 6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and Import Necessary Libraries\n"
      ],
      "metadata": {
        "id": "Jcu9V4ZUlc_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CUFLCRxlYnn",
        "outputId": "bed3e260-71e3-4930-f2c2-c68c914c53b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Training/DTS_Tensorflow/demo/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMxDRY9UjBPi",
        "outputId": "4244f534-06f1-490d-b3b1-74cf2a33cbd2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Training/DTS_Tensorflow/demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p checkpoint_model_word2vec\n",
        "%mkdir -p checkpoint_model_glove\n",
        "%mkdir -p checkpoint_model_bert"
      ],
      "metadata": {
        "id": "54ZPk8X-qdOd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpWFcVOBjXHj",
        "outputId": "ea338095-0b55-46ea-ccf4-69386b32a38d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.47.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy"
      ],
      "metadata": {
        "id": "Vzl1AD-ijJYT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Text Data"
      ],
      "metadata": {
        "id": "sYZvJ2wrlUYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('tweets.csv')"
      ],
      "metadata": {
        "id": "FLyqt932lvPQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+')\n",
        "    return url.sub(r' httpsmark ', text)\n",
        "\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_atsymbol(text):\n",
        "    name = re.compile(r'@\\S+')\n",
        "    return name.sub(r' atsymbol ', text)\n",
        "\n",
        "\n",
        "def remove_hashtag(text):\n",
        "    hashtag = re.compile(r'#')\n",
        "    return hashtag.sub(r' hashtag ', text)\n",
        "\n",
        "\n",
        "def remove_exclamation(text):\n",
        "    exclamation = re.compile(r'!')\n",
        "    return exclamation.sub(r' exclamation ', text)\n",
        "\n",
        "\n",
        "def remove_question(text):\n",
        "    question = re.compile(r'?')\n",
        "    return question.sub(r' question ', text)\n",
        "\n",
        "\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "\n",
        "def remove_number(text):\n",
        "    number = re.compile(r'\\d+')\n",
        "    return number.sub(r' number ', text)\n",
        "\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r' emoji ', string)"
      ],
      "metadata": {
        "id": "Z389IxoAlxRs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['text'] = df['text'].apply(lambda text: remove_URL(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_html(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_atsymbol(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_hashtag(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_exclamation(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_punc(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_number(text))\n",
        "df['text'] = df['text'].apply(lambda text: remove_emoji(text))"
      ],
      "metadata": {
        "id": "pC6Xtt6cl0SX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Word2Vec Pretrained From Wiki as Base Model"
      ],
      "metadata": {
        "id": "0bd_Wd5yl_09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gunzip cc.en.300.vec.gz\n",
        "!rm -rf gunzip cc.en.300.vec.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVQIllBdmVg8",
        "outputId": "4443af7a-7fbb-4afa-b096-deb3fc159d2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-26 08:46:07--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  40.9MB/s    in 35s     \n",
            "\n",
            "2022-07-26 08:46:42 (36.0 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(lower=True, filters='')\n",
        "tk.fit_on_texts(df.text)\n",
        "\n",
        "max_len = 280 # max twitter char\n",
        "train_tokenized = tk.texts_to_sequences(df.text)\n",
        "X = pad_sequences(train_tokenized, maxlen=max_len)"
      ],
      "metadata": {
        "id": "JW_4oEHYmGVb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,df.target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "eCNKlMvjmMa9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 300\n",
        "max_features = 50000\n",
        "\n",
        "def get_coefs(word,*arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(\"cc.en.300.vec\"))\n",
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "7e2Zk2n9mPxL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.001)\n",
        "\n",
        "model_word2vec = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape = (max_len,)),\n",
        "    tf.keras.layers.Embedding(nb_words+1, embed_size, weights = [embedding_matrix], trainable=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout=0.4)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model_word2vec.compile(loss=SparseCategoricalCrossentropy(),optimizer=adam,metrics=[SparseCategoricalAccuracy()])\n",
        "\n",
        "\n",
        "model_word2vec.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVgebXGyoP7I",
        "outputId": "3f2810ea-a4e0-4d26-fc38-ff9ef22ee9d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 280, 300)          7230900   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              186880    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,421,974\n",
            "Trainable params: 7,421,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = [EarlyStopping(\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    min_delta=0.01\n",
        "), \n",
        "ModelCheckpoint(\n",
        "    filepath='checkpoint_model_word2vec/',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "jj3zs_R7obMP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "history = model_word2vec.fit(X_train,y_train,batch_size=512,epochs=20, validation_split=0.2,callbacks=callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRs2QuHOonWF",
        "outputId": "8e8a267c-dca5-4b75-d193-32e532664123"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 50s 3s/step - loss: 0.5559 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.4616 - val_sparse_categorical_accuracy: 0.8121\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.4167 - sparse_categorical_accuracy: 0.8121 - val_loss: 0.3756 - val_sparse_categorical_accuracy: 0.8346\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.3054 - sparse_categorical_accuracy: 0.8844 - val_loss: 0.3351 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 37s 3s/step - loss: 0.2165 - sparse_categorical_accuracy: 0.9214 - val_loss: 0.3346 - val_sparse_categorical_accuracy: 0.8604\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.1266 - sparse_categorical_accuracy: 0.9604 - val_loss: 0.3563 - val_sparse_categorical_accuracy: 0.8764\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 33s 2s/step - loss: 0.0627 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.3990 - val_sparse_categorical_accuracy: 0.8714\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.4568 - val_sparse_categorical_accuracy: 0.8692\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 36s 2s/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.5187 - val_sparse_categorical_accuracy: 0.8643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_word2vec.load_weights(f'checkpoint_model_word2vec/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG5WCJJfopxa",
        "outputId": "c8a05707-d80e-4761-95f1-31229ab7bfa7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb7821be510>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_word2vec.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhX7rJY6orU8",
        "outputId": "931e3d35-4d9b-4b4b-c30b-6d14730bc633"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 10s 143ms/step - loss: 0.3249 - sparse_categorical_accuracy: 0.8879\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32491618394851685, 0.8878628015518188]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Glove Pretrained from Twitter Dataset"
      ],
      "metadata": {
        "id": "ejfSY3TFo4Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text,df.target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qR-8GlDQo323"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train = pad_sequences(sequences_train, maxlen=280, truncating='pre')\n",
        "X_test = pad_sequences(sequences_test, maxlen=280, truncating='pre')\n",
        "\n",
        "vocabSize = len(tokenizer.index_word) + 1\n",
        "print(f\"Vocabulary size = {vocabSize}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMD0niI5pH4G",
        "outputId": "72a04719-ceff-41a3-b006-69de0dd58cc2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size = 24103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read GloVE embeddings\n",
        "path_to_glove_file = 'glove.twitter.27B.200d.txt'\n",
        "num_tokens = vocabSize \n",
        "embedding_dim = 200\n",
        "hits = 0\n",
        "misses = 0\n",
        "embeddings_index = {}\n",
        "\n",
        "# Read word vectors\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "\n",
        "# Assign word vectors to our dictionary/vocabulary\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hifm1zDpM-C",
        "outputId": "e633dc78-9c80-487f-e676-f4af4e7e1c73"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1193514 word vectors.\n",
            "Converted 16715 words (7387 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build neural network architecture\n",
        "adam = Adam(learning_rate=0.001)\n",
        "\n",
        "model_glove = Sequential([\n",
        "    Embedding(vocabSize, 200, weights=[embedding_matrix], trainable=False,input_length=280),\n",
        "    Bidirectional(LSTM(64,recurrent_dropout=0.4)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model_glove.compile(loss=SparseCategoricalCrossentropy(),optimizer=adam,metrics=[SparseCategoricalAccuracy()])\n",
        "\n",
        "\n",
        "model_glove.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9edyDLLpOta",
        "outputId": "3d8d4ca2-9955-4795-c5a6-5ecbbb0e1817"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 280, 200)          4820600   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              135680    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,960,474\n",
            "Trainable params: 139,874\n",
            "Non-trainable params: 4,820,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Callback\n",
        "callback = [EarlyStopping(\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    min_delta=0.01\n",
        "), \n",
        "ModelCheckpoint(\n",
        "    filepath='checkpoint_model_glove/',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "we6LVfSspSfR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "history = model_glove.fit(X_train,y_train,batch_size=512,epochs=20, validation_split=0.2,callbacks=callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEA5B0CHpU8Q",
        "outputId": "10384dd1-421c-4d6b-f503-62c6990b8143"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 35s 2s/step - loss: 0.6146 - sparse_categorical_accuracy: 0.6739 - val_loss: 0.4792 - val_sparse_categorical_accuracy: 0.8121\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 35s 2s/step - loss: 0.4595 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.4276 - val_sparse_categorical_accuracy: 0.8121\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 29s 2s/step - loss: 0.3945 - sparse_categorical_accuracy: 0.8208 - val_loss: 0.3592 - val_sparse_categorical_accuracy: 0.8440\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 29s 2s/step - loss: 0.3400 - sparse_categorical_accuracy: 0.8520 - val_loss: 0.3236 - val_sparse_categorical_accuracy: 0.8725\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 38s 3s/step - loss: 0.3114 - sparse_categorical_accuracy: 0.8726 - val_loss: 0.3040 - val_sparse_categorical_accuracy: 0.8764\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.2925 - sparse_categorical_accuracy: 0.8769 - val_loss: 0.3023 - val_sparse_categorical_accuracy: 0.8758\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 29s 2s/step - loss: 0.2835 - sparse_categorical_accuracy: 0.8844 - val_loss: 0.2893 - val_sparse_categorical_accuracy: 0.8830\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 29s 2s/step - loss: 0.2713 - sparse_categorical_accuracy: 0.8910 - val_loss: 0.2854 - val_sparse_categorical_accuracy: 0.8830\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.2649 - sparse_categorical_accuracy: 0.8931 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.8868\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.2570 - sparse_categorical_accuracy: 0.8964 - val_loss: 0.2775 - val_sparse_categorical_accuracy: 0.8868\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 35s 2s/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.2781 - val_sparse_categorical_accuracy: 0.8874\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 29s 2s/step - loss: 0.2413 - sparse_categorical_accuracy: 0.9016 - val_loss: 0.2986 - val_sparse_categorical_accuracy: 0.8802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_glove.load_weights(f'checkpoint_model_glove/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7PXbR77pXOp",
        "outputId": "6d062a04-3ddf-47f4-ca59-b23e4623e6e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb65a48f810>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_glove.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZqSHP1MpcYr",
        "outputId": "89d7f94c-75f2-4b5a-ec0d-cf3fd9693b58"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 14s 191ms/step - loss: 0.2690 - sparse_categorical_accuracy: 0.8953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2689626216888428, 0.8953385949134827]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Pretrained BERT"
      ],
      "metadata": {
        "id": "C4wpzLDbplKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text,df.target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "kG73YKdPpkVl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
      ],
      "metadata": {
        "id": "1N0yK2-Rp0lG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "preprocessed_text = bert_preprocess(text_input)\n",
        "outputs = bert_encoder(preprocessed_text)"
      ],
      "metadata": {
        "id": "O22Swg-6p2Bs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.Dense(32, activation='relu')(outputs['pooled_output'])\n",
        "layer = tf.keras.layers.Dropout(0.5, name=\"dropout\")(layer)\n",
        "layer = tf.keras.layers.Dense(2, activation='softmax')(layer)"
      ],
      "metadata": {
        "id": "oY6Tv28jp3sO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bert = tf.keras.Model(inputs=[text_input], outputs = [layer])\n",
        "model_bert.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQThVEBFp8js",
        "outputId": "f95469ac-a3f4-4cf9-8daa-a7074cb324a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['keras_layer[0][0]',            \n",
            "                                 (None, 128, 768),                'keras_layer[0][1]',            \n",
            "                                 'encoder_outputs':               'keras_layer[0][2]']            \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'default': (None,                                                \n",
            "                                768)}                                                             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 32)           24608       ['keras_layer_1[0][13]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 32)           0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 2)            66          ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,506,915\n",
            "Trainable params: 24,674\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.001)\n",
        "\n",
        "model_bert.compile(loss=SparseCategoricalCrossentropy(),optimizer=adam,metrics=[SparseCategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "ATzUf47Pp-4B"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = [EarlyStopping(\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    min_delta=0.01\n",
        "), \n",
        "ModelCheckpoint(\n",
        "    filepath='checkpoint_model_bert/',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "dYGiPSrHqFpa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_bert.fit(X_train,y_train,batch_size=512,epochs=20, validation_split=0.2,callbacks=callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UqAosmcqHBS",
        "outputId": "6839d5b0-6b9d-457d-d09b-a772ff9fe308"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 218s 14s/step - loss: 0.5415 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4493 - val_sparse_categorical_accuracy: 0.8121\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 204s 14s/step - loss: 0.4685 - sparse_categorical_accuracy: 0.8106 - val_loss: 0.4343 - val_sparse_categorical_accuracy: 0.8121\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 206s 14s/step - loss: 0.4545 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.4183 - val_sparse_categorical_accuracy: 0.8121\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 205s 14s/step - loss: 0.4410 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.4267 - val_sparse_categorical_accuracy: 0.8126\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 206s 14s/step - loss: 0.4400 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.4093 - val_sparse_categorical_accuracy: 0.8132\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 204s 14s/step - loss: 0.4302 - sparse_categorical_accuracy: 0.8110 - val_loss: 0.4028 - val_sparse_categorical_accuracy: 0.8126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bert.load_weights(f'checkpoint_model_bert/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTN8PkV9qLFY",
        "outputId": "fc84d65c-602f-4e80-f84d-e48e870a8e50"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb78348dc10>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bert.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJVLJfebqMhm",
        "outputId": "c1200a3f-907d-4ea8-f695-92182c01e4c5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 22s 307ms/step - loss: 0.3957 - sparse_categorical_accuracy: 0.8254\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3956999182701111, 0.8254177570343018]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hfMtQ16hvo6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}